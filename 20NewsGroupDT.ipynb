{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedHamood/20NewsGroup/blob/main/20NewsGroupDT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nGPW7BV152e"
      },
      "source": [
        "# 20NewsGroup - Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1UHEjaVFgwM"
      },
      "source": [
        "MEMO: Include any decisions about training/validation split, regularization\n",
        "strategies, any optimization tricks, setting hyper-parameters, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e89IS-aq2AKz"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "YATSwO2luyBW",
        "outputId": "690a94c8-d5cb-444f-b7c9-f74e785bcf8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\umroot\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\umroot\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\umroot\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREPROCESSING ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b' ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREPROCESSING DONE!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pipelines_FEngineering as BaseLine\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import preprocessingNLP as PNLP\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.decomposition import PCA, NMF,TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Import Dataset\n",
        "TNGD_Train = fetch_20newsgroups(subset='train',shuffle=True, random_state=42,remove=['headers', 'footers', 'quotes'])\n",
        "TNGD_test = fetch_20newsgroups(subset='test',shuffle=True, random_state=42,remove=['headers', 'footers', 'quotes'])\n",
        "\n",
        "# Preprocessing\n",
        "print(\"PREPROCESSING ...\")\n",
        "TNGD_Train.data = PNLP.customNLP(TNGD_Train.data)\n",
        "TNGD_test.data = PNLP.customNLP(TNGD_test.data)\n",
        "TNGD_Train.data, TNGD_Train.target = PNLP.removeEmptyInstances(TNGD_Train.data, TNGD_Train.target)\n",
        "\n",
        "\n",
        "\n",
        "print(\"PREPROCESSING DONE!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxUPi72X4P2u"
      },
      "outputs": [],
      "source": [
        "class dcdistanceTransformer(TransformerMixin, BaseEstimator): \n",
        "        \"\"\"\n",
        "        proposed method modifed from(DCDistance: A Supervised Text Document Feature extraction based on class labels)\n",
        "        \"\"\"\n",
        "        def __init__(self):\n",
        "            super(dcdistanceTransformer, self).__init__()\n",
        "            \n",
        "\n",
        "        def fit(self, X, y, **fit_params):\n",
        "            self.alpha=20\n",
        "            self.Classes=np.unique(y, return_index = False)\n",
        "            self.split=np.linspace(0, X.shape[1],self.alpha,endpoint = False,  dtype=int)\n",
        "            self.vd=np.zeros((len(self.Classes),X.shape[1]))\n",
        "            self.vd2=np.zeros((len(self.Classes),X.shape[1]))\n",
        "            for i in self.Classes:\n",
        "                ind = np.where(y == i)[0]\n",
        "                temp=np.zeros((1,X.shape[1]))\n",
        "                for feat in range(0,len(self.split)):\n",
        "                    ind1=self.split[feat]              \n",
        "                    ind2=self.split[feat]+self.split[1]\n",
        "                    temp=X.tocsr()[ind,ind1:ind2].sum(axis = 0)\n",
        "                    self.vd2[int(i),ind1:ind2]=temp     \n",
        "            tfidf_transformer = TfidfTransformer()\n",
        "            self.vd2 = tfidf_transformer.fit_transform(self.vd2) \n",
        "            self.vd2=self.vd2.todense()   \n",
        "            return self\n",
        "\n",
        "        def transform(self, X, **fit_params):\n",
        "            V= np.zeros((X.shape[0],len(self.Classes)))\n",
        "\n",
        "            \n",
        "            for i in self.Classes:\n",
        "                starting=0\n",
        "                ending=200\n",
        "                while starting<X.shape[0]-2:\n",
        "                    if ending>X.shape[0]:\n",
        "                        ending=X.shape[0]\n",
        "                    temp=np.linalg.norm(X.tocsr()[starting:ending,:]-self.vd2[int(i),:],axis=1,ord=2)\n",
        "                    V[starting:ending,int(i)]= temp[:] #('Norm',Normalizer(copy=False)),\n",
        "                    starting=ending\n",
        "                    ending=ending+200\n",
        "            V= sparse.csr_matrix(V)\n",
        "            X=[]\n",
        "            return V         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6wa9jeg1-PD"
      },
      "source": [
        "## Cross-Validation Splitting Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4JezyHUGiRr"
      },
      "source": [
        "Let's first analyse how the cross-validation splitting strategy makes an impact on the training accuracy of the dataset using Logistic Regression classifier with its default parameters. In the following experiment, Logistic Regression will be applied on the dataset for different number of folds (5 to 15). The elapsed time will also be collected for each training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "xvFH3wDJ2HV0",
        "outputId": "534adb05-21cc-476e-e565-136d4672f1c4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'RandomizedSearchCV' object has no attribute 'best_score_'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-2d8d5742347f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m   \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDT_CV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m   \u001b[0mfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mruntimes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score_'"
          ]
        }
      ],
      "source": [
        "# Decision Tree Model\n",
        "accuracies = []\n",
        "folds = []\n",
        "runtimes = []\n",
        "parameters={}\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('redu',dcdistanceTransformer),('Norm', Normalizer(copy=False)),\n",
        "                ('clf', DecisionTreeClassifier())])\n",
        "for i in range(6,13):\n",
        "  start = time.perf_counter()\n",
        "  DT_CV = RandomizedSearchCV(pip, parameters, cv=i, n_jobs=-1)\n",
        "  end = time.perf_counter()\n",
        "  elapsed_time = end-start\n",
        "  accuracies.append(DT_CV.best_score_)\n",
        "  folds.append(i)\n",
        "  runtimes.append(elapsed_time)\n",
        "\n",
        "#Display Results\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax1.plot(folds, accuracies, 'o-')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_xlabel('Fold Size')\n",
        "ax2.plot(folds, runtimes, 'o-')\n",
        "ax2.set_ylabel('Training Time')\n",
        "ax2.set_xlabel('Fold Size')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7MPHe58FZ8T"
      },
      "source": [
        "The first graph shows that within a range of 5 to 15 folds, a 11-fold cross-validation gives the best accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxrwHBflzAmx"
      },
      "source": [
        "## Setting Hyper-Parameters-Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2THj0rFt8Y4x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set relevant parameters\n",
        "parameters = {\n",
        "    'clf__criterion': ('gini','entropy'),\n",
        "    'clf__ccp_alpha': (0,.05,.1 ,.5,1,5),\n",
        "\n",
        "    }\n",
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('redu',dcdistanceTransformer),('Norm', Normalizer(copy=False)),\n",
        "                ('clf', DecisionTreeClassifier())])\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "n_iter_search=8\n",
        "cv_folds=10\n",
        "DT_pruning = RandomizedSearchCV(pip, param_distributions=parameters, \n",
        "                               n_iter=n_iter_search, cv=cv_folds)\n",
        "\n",
        "# Execute RandomizedSearchCV and print best results \n",
        "DT_pruning.fit(TNGD_Train.data, TNGD_Train.target)\n",
        "report(DT_pruning.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_WzzOxc4P2x"
      },
      "outputs": [],
      "source": [
        "## Setting Hyper-Parameters early stoping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZGnJDTG4P2x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set relevant parameters\n",
        "parameters = {\n",
        "    'clf__criterion': ('gini','entropy'),\n",
        "    #'clf__ccp_alpha': (0,.05,.1 ,.5,1,5),\n",
        "    'clf__max_depth': (None,5,10,20),\n",
        "    'clf__min_samples_split': (2,8,10,50,100),\n",
        "    'clf__min_samples_leaf': (1,10,50,100),\n",
        "    'clf__max_features': (None,'auto', 'sqrt', 'log2'),\n",
        "#    'clf__max_leaf_nodes': (None,20,30,40,80),\n",
        "    }\n",
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('redu',dcdistanceTransformer),('Norm', Normalizer(copy=False)),\n",
        "                ('clf', DecisionTreeClassifier())])\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "n_iter_search=10\n",
        "cv_folds=10\n",
        "DT_early_stoping = RandomizedSearchCV(pip, param_distributions=parameters, \n",
        "                               n_iter=n_iter_search, cv=cv_folds)\n",
        "\n",
        "# Execute RandomizedSearchCV and print best results \n",
        "DT_early_stoping.fit(TNGD_Train.data, TNGD_Train.target)\n",
        "report(DT_early_stoping.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DTXaN_O9gZO"
      },
      "source": [
        "In the following section, GridSearchCV is used to properly detect the most optimal parameters needed to optimize the validation accuracy of Logistic Regression. \n",
        "\n",
        "The previous results help to properly select the hyper-parameters. It seems that the default tolerance of 0.0001 is a good value for stochastic gradient descent. It also seems that a higher validation accuracy is obtained when no penalty is applied on the training. Although this might look good, it could also be a sign of overfitting, which could then generate a poor test accuracy. To avoid this issue, Lasso Regression (L1 penalty) is applied with an appropriate regularization strength. Indeed, this type of regression works the best because it produces sparse weights, which helps with feature selection by reducing the amount of features in the provided design matrix. This eventually reduces the complexity of the model. Furthermore, every test was able to find the minimum error without reaching the maximum iteration provided in the paremeters. Therefore, 300 iterations seems like an appropriate maximum value. Finally, using GridSearchCV helps choosing the appropriate regularization strength for the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIVoOE82B_a_"
      },
      "outputs": [],
      "source": [
        "# Set relevant parameters\n",
        "parameters = {\n",
        "    'clf__C': ( 2.0,5, 10,15)\n",
        "}\n",
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range, max_features=max_features)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),\n",
        "                ('clf', LogisticRegression(max_iter=300, penalty='l2',tol=.001, solver='saga', n_jobs=-1))])\n",
        "\n",
        "# Initialize and execute GridSearchCV\n",
        "gs_clf = GridSearchCV(pip, parameters, cv=cv_folds, n_jobs=-1)\n",
        "start_time = time.perf_counter()\n",
        "gs_clf = gs_clf.fit(TNGD_Train.data, TNGD_Train.target)\n",
        "elapsed_time = time.perf_counter() - start_time\n",
        "\n",
        "# Print results\n",
        "print(\"Validation Accuracy: \" + str(gs_clf.best_score_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_estimator_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_params_))\n",
        "print(\"\")\n",
        "\n",
        "train_accuracies = gs_clf.cv_results_['mean_test_score']\n",
        "train_time = gs_clf.cv_results_['mean_fit_time']\n",
        "train_params = gs_clf.cv_results_['params']\n",
        "\n",
        "for i in range(len(train_time)):\n",
        "  print(\"Parameter: {0}\".format(train_params[i]))\n",
        "  print(\"Training Time: %.3f seconds\" %train_time[i])\n",
        "  print(\"Valdation Accuracy: {0:.3f}\".format(train_accuracies[i]))\n",
        "  print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDby7poK4P2y"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_pridect=gs_clf.predict(TNGD_test.data)\n",
        "print(classification_report(test_pridect, TNGD_test.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2h4dTIj4P2y"
      },
      "outputs": [],
      "source": [
        "print(np.mean(test_pridect==TNGD_test.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N0TMaOy4P2y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}