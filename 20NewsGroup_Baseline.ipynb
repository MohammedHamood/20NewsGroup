{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedHamood/20NewsGroup/blob/main/20NewsGroup_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q92DEmBM1b8_"
      },
      "source": [
        "# 20 News Group - Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiMg7PRyGZ_F"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZaX3DOsGgF6",
        "outputId": "d1e2b4fd-4afc-462e-a530-2ff0cb3f145e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import preprocessingNLP as PNLP\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Import Dataset\n",
        "print(\"Fetching Dataset ...\")\n",
        "!wget -nv \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "Newsgroup_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=['headers', 'footers', 'quotes'])\n",
        "Newsgroup_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=['headers', 'footers', 'quotes'])\n",
        "print(\"Dataset Fetched\")\n",
        "\n",
        "# Preprocessing\n",
        "print(\"PREPROCESSING ...\")\n",
        "Newsgroup_train.data = PNLP.customNLP(Newsgroup_train.data)\n",
        "Newsgroup_test.data = PNLP.customNLP(Newsgroup_test.data)\n",
        "Newsgroup_train.data, Newsgroup_train.target = PNLP.removeEmptyInstances(Newsgroup_train.data, Newsgroup_train.target)\n",
        "print(\"PREPROCESSING DONE!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Fetching Dataset ...\n",
            "2020-03-11 05:52:03 URL:http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz [84125825/84125825] -> \"aclImdb_v1.tar.gz\" [1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset Fetched\n",
            "PREPROCESSING ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PREPROCESSING DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhinXDrn5JFP"
      },
      "source": [
        "## Baseline for All Models\n",
        "Performs cross validation for all models with default params to obtain baseline accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZtX0CYapx8n"
      },
      "source": [
        "### Logistic Regression\n",
        "Default Parameters: <br />\n",
        "penalty: l2 <br />\n",
        "tol: 1e-4 <br />\n",
        "C: 1 <br />\n",
        "solver: lbfgs (only handles l2 or no penalty) <br />\n",
        "max_iter: 100 <br />\n",
        "n_jobs: None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THTnP49Pp3Kf",
        "outputId": "1ebc8320-588b-4174-f59f-2c473a4d49eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', LogisticRegression())])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)\n",
        "print(\"Train_Acc: %s\" % (scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 318.0961935520172 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7569608735213831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1sNHUXpsp2"
      },
      "source": [
        "### SVM\n",
        "Default params: <br />\n",
        "penalty: l2 <br />\n",
        "loss: squared_hing <br />\n",
        "tol: 1e-4 <br />\n",
        "C: 1 <br />\n",
        "multi_class: 'ovr' <br />\n",
        "max_iter: 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2VQeiEIG--6",
        "outputId": "bdffb515-ef46-4c48-9025-d2ba6fa54df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', LinearSVC())])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 22.728375911712646 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7778889899909008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGSaZg4eqHJU"
      },
      "source": [
        "### Decision Trees\n",
        "criterion: 'gini' <br />\n",
        "splitter: 'best' <br />\n",
        "max_depth: 'None' <br />\n",
        "min_samples_split: 2 <br />\n",
        "min_samples_leaf: 1 <br />\n",
        "max_features: None <br />\n",
        "max_leaf_nodes: None <br />\n",
        "ccp_alpha: 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAr5mUVtqjPv",
        "outputId": "8fe25530-73ee-4403-9e3f-8c144ae3d304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', DecisionTreeClassifier())])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 152.30219268798828 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4871701546860782"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHmjx7YAqrqa"
      },
      "source": [
        "### Random Forest\n",
        "n_estimator: 100 <br />\n",
        "criterion: 'gini' <br />\n",
        "max_depth: None <br />\n",
        "min_samples_split: 2 <br />\n",
        "min_samples_leaf: 1 <br />\n",
        "max_features: 'auto' <br />\n",
        "max_leaf_nodes: None <br />\n",
        "n_jobs: None <br />\n",
        "ccp_alpha: 0.0 <br />\n",
        "max_samples: None <br />\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqJ-ebIKqv4u",
        "outputId": "a87044d1-1b79-4dd8-ad07-2f6e3a530434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', RandomForestClassifier())])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=3)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 86.77589225769043 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6577802743195756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayGRuZdGq2FD"
      },
      "source": [
        "### RF with Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX_dPvXRq6U7",
        "outputId": "10576a77-b33a-4f76-b6db-815b04e6b9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "\n",
        "\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('clf', AdaBoostClassifier(RandomForestClassifier(n_jobs=-1)))])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=3)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 1779.6300780773163 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5797098954850046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhw5U2m4vTUn"
      },
      "source": [
        "### DT with Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwaJD8tcvSH9",
        "outputId": "3deabc3f-aa8a-4759-e433-97f9b9092c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', DecisionTreeClassifier())])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 140.89277458190918 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4878070973612375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHoRPHAhsT6u"
      },
      "source": [
        "### SVM with Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf0ZuIpxs4Ed",
        "outputId": "3ebe7c2a-52e2-40cc-d042-287fd8ba3ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', AdaBoostClassifier(LinearSVC(), algorithm='SAMME'))])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 427.1325800418854 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5448589626933575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjjC98dSuZqA"
      },
      "source": [
        "### LR with Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNb3Fdpue0S",
        "outputId": "a77c383f-eac6-486a-a5c7-2e631f2f4e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create a pipeline\n",
        "pip = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
        "                ('Norm', Normalizer(copy=False)),('clf', AdaBoostClassifier(LogisticRegression()))])\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "scores = cross_val_score(pip, Newsgroup_train.data, Newsgroup_train.target, cv=10)\n",
        "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 927.720205783844 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4714285714285714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}