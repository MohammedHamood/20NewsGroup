{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedHamood/20NewsGroup/blob/main/20newsdataset_DCdistance_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTb_N-H3KU0"
      },
      "source": [
        "# Perform DCdicetance alorithm for all models assuming no need for regularization and no need for early stoping as the number of features are very low equals to k."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pmw_Op_3KU8"
      },
      "source": [
        "## Loading dataBase and initializtion process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqKpT58n3KVB",
        "outputId": "f8176c40-34ae-4cd2-d5e7-0a10f74d24e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pipelines_FEngineering as BaseLine\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import preprocessingNLP as PNLP\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.decomposition import PCA, NMF,TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Import Dataset\n",
        "TNGD_Train = fetch_20newsgroups(subset='train',shuffle=True, random_state=42,remove=['headers', 'footers', 'quotes'])\n",
        "TNGD_test = fetch_20newsgroups(subset='test',shuffle=True, random_state=42,remove=['headers', 'footers', 'quotes'])\n",
        "\n",
        "# Preprocessing\n",
        "print(\"PREPROCESSING ...\")\n",
        "TNGD_Train.data = PNLP.customNLP(TNGD_Train.data)\n",
        "TNGD_test.data = PNLP.customNLP(TNGD_test.data)\n",
        "TNGD_Train.data, TNGD_Train.target = PNLP.removeEmptyInstances(TNGD_Train.data, TNGD_Train.target)\n",
        "resample=np.linspace(0,TNGD_Train.target.size,TNGD_Train.target.size,endpoint=False, retstep=False, dtype=int)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "resample, TNGD_Train.target =ros.fit_resample(resample[:,None], TNGD_Train.target)\n",
        "data=[]\n",
        "for i in resample:\n",
        "    data.append(TNGD_Train.data[int(i)])\n",
        "TNGD_Train.data=data\n",
        "data=[]\n",
        "resample=[] \n",
        "\n",
        "\n",
        "print(\"PREPROCESSING DONE!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PREPROCESSING ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PREPROCESSING DONE!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwtI2g4_3KVG"
      },
      "source": [
        "# needed functions "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1H-1VlF3KVi"
      },
      "source": [
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=10):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
        "                  .format(results['mean_test_score'][candidate],\n",
        "                          results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"Mean Fit Time: %.3f seconds\" %results['mean_fit_time'][candidate])\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "from scipy import sparse\n",
        "def Pipeline_FeatureEngineering(train_data,train_target,parameters=None,CV=10, \n",
        "                                reductionMethod=PCA(),reductionType=1,model=LinearSVC()):\n",
        "    class DenseTransformer(TransformerMixin):\n",
        "\n",
        "        def fit(self, X, y=None, **fit_params):\n",
        "            return self\n",
        "\n",
        "        def transform(self, X, y=None, **fit_params):\n",
        "            return X.todense()\n",
        "    class sparseTransformer(TransformerMixin):\n",
        "\n",
        "        def fit(self, X, y=None, **fit_params):\n",
        "            return self\n",
        "\n",
        "        def transform(self, X, y=None, **fit_params):\n",
        "            return sparse.csr_matrix(X) \n",
        "        \n",
        " \n",
        "\n",
        "    class dcdistanceTransformer(TransformerMixin, BaseEstimator): \n",
        "        \"\"\"\n",
        "        proposed method modifed from(DCDistance: A Supervised Text Document Feature extraction based on class labels)\n",
        "        \"\"\"\n",
        "        def __init__(self):\n",
        "            super(dcdistanceTransformer, self).__init__()\n",
        "            \n",
        "\n",
        "        def fit(self, X, y, **fit_params):\n",
        "            self.alpha=20\n",
        "            self.Classes=np.unique(y, return_index = False)\n",
        "            self.split=np.linspace(0, X.shape[1],self.alpha,endpoint = False,  dtype=int)\n",
        "            self.vd=np.zeros((len(self.Classes),X.shape[1]))\n",
        "            self.vd2=np.zeros((len(self.Classes),X.shape[1]))\n",
        "            for i in self.Classes:\n",
        "                ind = np.where(y == i)[0]\n",
        "                temp=np.zeros((1,X.shape[1]))\n",
        "                for feat in range(0,len(self.split)):\n",
        "                    ind1=self.split[feat]              \n",
        "                    ind2=self.split[feat]+self.split[1]\n",
        "                    temp=X.tocsr()[ind,ind1:ind2].sum(axis = 0)\n",
        "                    self.vd2[int(i),ind1:ind2]=temp     \n",
        "            tfidf_transformer = TfidfTransformer()\n",
        "            self.vd2 = tfidf_transformer.fit_transform(self.vd2) \n",
        "            self.vd2=self.vd2.todense()   \n",
        "            return self\n",
        "\n",
        "        def transform(self, X, **fit_params):\n",
        "            V= np.zeros((X.shape[0],len(self.Classes)))\n",
        "\n",
        "            \n",
        "            for i in self.Classes:\n",
        "                starting=0\n",
        "                ending=200\n",
        "                while starting<X.shape[0]-2:\n",
        "                    if ending>X.shape[0]:\n",
        "                        ending=X.shape[0]\n",
        "                    temp=np.linalg.norm(X.tocsr()[starting:ending,:]-self.vd2[int(i),:],axis=1,ord=2)\n",
        "                    V[starting:ending,int(i)]= temp[:] #('Norm',Normalizer(copy=False)),\n",
        "                    starting=ending\n",
        "                    ending=ending+200\n",
        "            V= sparse.csr_matrix(V)\n",
        "            X=[]\n",
        "            return V         \n",
        "#\n",
        "        \n",
        "    if reductionType==1:\n",
        "        text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),('tfidf', TfidfTransformer()),\n",
        "                             ('redu',dcdistanceTransformer()),\n",
        "                             ('Norm',Normalizer(copy=False)),('clf', model)])\n",
        "    elif reductionType==2:\n",
        "        text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),('tfidf', TfidfTransformer()),\n",
        "                             ('redu',reductionMethod),\n",
        "                             ('Norm',Normalizer(copy=False)),('clf', model)])\n",
        "    elif reductionType==3:\n",
        "        text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),('tfidf', TfidfTransformer()),\n",
        "                             ('Norm1',Normalizer(copy=False)),('clf', model)])    \n",
        "    elif reductionType==0:\n",
        "        text_clf = Pipeline([('vect', CountVectorizer()),('samp',ADASYN(random_state=42)),('tfidf', TfidfTransformer()),\n",
        "                             ('redu1',TruncatedSVD(n_components=300)),('Norm1',Normalizer(copy=False)),('redu',reductionMethod),\n",
        "                             ('Norm2',Normalizer(copy=False)),('clf', model)])\n",
        "    gs_clf = GridSearchCV(text_clf, parameters, cv=CV, n_jobs=-1)\n",
        "    gs_clf = gs_clf.fit(train_data, train_target)\n",
        "    return gs_clf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu-A7ko73KVu"
      },
      "source": [
        "#setting defined parameters\n",
        "ngram_range=(1,2)\n",
        "cv_folds=11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4XmP1C3KV2"
      },
      "source": [
        "#1)logistic regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef5Kj3KY3KV6"
      },
      "source": [
        "# Set relevant parameters\n",
        "parameters = {\n",
        "     'clf__C': [2000],\n",
        "     #'clf__tol': (0.0001, 0.001)\n",
        "}\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "start_time = time.perf_counter()\n",
        "gs_clf= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=LogisticRegression(solver='saga',max_iter=1000))\n",
        "elapsed_time = time.perf_counter() - start_time\n",
        "\n",
        "# Print results\n",
        "print(\"Validation Accuracy: \" + str(gs_clf.best_score_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_estimator_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_params_))\n",
        "print(\"\")\n",
        "\n",
        "train_accuracies = gs_clf.cv_results_['mean_test_score']\n",
        "train_time = gs_clf.cv_results_['mean_fit_time']\n",
        "train_params = gs_clf.cv_results_['params']\n",
        "\n",
        "for i in range(len(train_time)):\n",
        "  print(\"Parameter: {0}\".format(train_params[i]))\n",
        "  print(\"Training Time: %.3f seconds\" %train_time[i])\n",
        "  print(\"Valdation Accuracy: {0:.3f}\".format(train_accuracies[i]))\n",
        "  print(\"\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LauTnG03KWB"
      },
      "source": [
        "#2)SVM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGtvzMHt3KWQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b3ccdbad-1dbf-4304-dc86-3e738c43e8a3"
      },
      "source": [
        "parameters = {\n",
        "     'clf__C': [2000],\n",
        "     #'clf__tol': (0.0001, 0.001)\n",
        "}\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "start_time = time.perf_counter()\n",
        "gs_clf= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=LinearSVC(max_iter=1000))\n",
        "elapsed_time = time.perf_counter() - start_time\n",
        "\n",
        "# Print results\n",
        "print(\"Validation Accuracy: \" + str(gs_clf.best_score_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_estimator_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_params_))\n",
        "print(\"\")\n",
        "\n",
        "train_accuracies = gs_clf.cv_results_['mean_test_score']\n",
        "train_time = gs_clf.cv_results_['mean_fit_time']\n",
        "train_params = gs_clf.cv_results_['params']\n",
        "\n",
        "for i in range(len(train_time)):\n",
        "  print(\"Parameter: {0}\".format(train_params[i]))\n",
        "  print(\"Training Time: %.3f seconds\" %train_time[i])\n",
        "  print(\"Valdation Accuracy: {0:.3f}\".format(train_accuracies[i]))\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b2610c7b71fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize gridsearchcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m SVM_base= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n\u001b[0;32m----> 9\u001b[0;31m                                          , reductionMethod=PCA(),reductionType=1,model=LinearSVC())\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Execute gridsearchcv and print best results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9f306cc0fd60>\u001b[0m in \u001b[0;36mPipeline_FeatureEngineering\u001b[0;34m(train_data, train_target, parameters, CV, reductionMethod, reductionType, model)\u001b[0m\n\u001b[1;32m     94\u001b[0m                              ('Norm2',Normalizer(copy=False)),('clf', model)])\n\u001b[1;32m     95\u001b[0m     \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRI7YZBG3KWf"
      },
      "source": [
        "cv_folds=10\n",
        "# Set relevant parameters\n",
        "parameters = {\n",
        "    'clf__min_samples_split': [100],\n",
        "    #'clf__max_features': ('sqrt', 'log2'),\n",
        "    #'clf__n_estimators': (80,200),\n",
        "    }\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "start_time = time.perf_counter()\n",
        "gs_clf= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=LinearSVC(max_iter=1000))\n",
        "elapsed_time = time.perf_counter() - start_time\n",
        "\n",
        "# Print results\n",
        "print(\"Validation Accuracy: \" + str(gs_clf.best_score_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_estimator_))\n",
        "print(\"Optimal Parameters: \" + str(gs_clf.best_params_))\n",
        "print(\"\")\n",
        "\n",
        "train_accuracies = gs_clf.cv_results_['mean_test_score']\n",
        "train_time = gs_clf.cv_results_['mean_fit_time']\n",
        "train_params = gs_clf.cv_results_['params']\n",
        "\n",
        "for i in range(len(train_time)):\n",
        "  print(\"Parameter: {0}\".format(train_params[i]))\n",
        "  print(\"Training Time: %.3f seconds\" %train_time[i])\n",
        "  print(\"Valdation Accuracy: {0:.3f}\".format(train_accuracies[i]))\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU7vwR3o3KWq"
      },
      "source": [
        "parameters = {\n",
        "    'clf__n_estimators': (8,50,80),\n",
        "    'clf__base_estimator':[LogisticRegression(solver='saga',max_iter=1000)],\n",
        "    'clf__base_estimator_C':(1.0,5,10, 100.0,1000,50),\n",
        "    }\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "ada_LR_base= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=AdaBoostClassifier())\n",
        "\n",
        "# Execute gridsearchcv and print best results \n",
        "\n",
        "report(ada_LR_base.cv_results_)\n",
        "\n",
        "\n",
        "test_pridect=ada_LR_base.predict(TNGD_test.data)\n",
        "print(classification_report(test_pridect, TNGD_test.target))\n",
        "\n",
        "print(np.mean(test_pridect==TNGD_test.target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-lXVzLe3KWu"
      },
      "source": [
        "parameters = {\n",
        "    'clf__n_estimators': (8,50,80),\n",
        "    'clf__base_estimator':[LinearSVC()],\n",
        "    'clf__base_estimator_C':(1.0,5,10, 100.0,1000,50),\n",
        "    }\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "ada_LR_base= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=AdaBoostClassifier())\n",
        "\n",
        "# Execute gridsearchcv and print best results \n",
        "\n",
        "report(ada_LR_base.cv_results_)\n",
        "\n",
        "\n",
        "test_pridect=ada_LR_base.predict(TNGD_test.data)\n",
        "print(classification_report(test_pridect, TNGD_test.target))\n",
        "\n",
        "print(np.mean(test_pridect==TNGD_test.target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_niVgDc3KWz"
      },
      "source": [
        "parameters = {\n",
        "    #'clf__min_samples_split': (200,100),\n",
        "    #'clf__max_features': ('sqrt', 'log2'),\n",
        "    'clf__n_estimators': (10,50,100),\n",
        "    }\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "RF_base= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=RandomForestClassifier(max_features='sqrt'))\n",
        "\n",
        "# Execute gridsearchcv and print best results \n",
        "\n",
        "report(RF_base.cv_results_)\n",
        "\n",
        "\n",
        "test_pridect=RF_base.predict(TNGD_test.data)\n",
        "print(classification_report(test_pridect, TNGD_test.target))\n",
        "\n",
        "print(np.mean(test_pridect==TNGD_test.target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXae67s3KW3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvorQKtb3KXM"
      },
      "source": [
        "parameters = {\n",
        "    'clf__n_estimators': (8,50,80),\n",
        "    'clf__base_estimator':[RandomForestClassifier(max_features='sqrt')],\n",
        "    }\n",
        "\n",
        "# Initialize gridsearchcv\n",
        "ada_RF_base= Pipeline_FeatureEngineering(TNGD_Train.data,TNGD_Train.target,parameters,CV=cv_folds\n",
        "                                         , reductionMethod=PCA(),reductionType=1,model=AdaBoostClassifier())\n",
        "\n",
        "# Execute gridsearchcv and print best results \n",
        "\n",
        "report(ada_RF_base.cv_results_)\n",
        "\n",
        "\n",
        "test_pridect=ada_RF_base.predict(TNGD_test.data)\n",
        "print(classification_report(test_pridect, TNGD_test.target))\n",
        "\n",
        "print(np.mean(test_pridect==TNGD_test.target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZSctD8M3KXV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}